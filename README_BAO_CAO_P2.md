# ğŸ“š BÃO CÃO Äá»’ ÃN Tá»T NGHIá»†P - PHáº¦N 2
## Há»‡ Thá»‘ng PhÃ¡t Hiá»‡n Táº¥n CÃ´ng DDoS Sá»­ Dá»¥ng Federated Learning

---

## ğŸ“‹ Má»¤C Lá»¤C PHáº¦N 2
6. [Quy TrÃ¬nh Hoáº¡t Äá»™ng](#6-quy-trÃ¬nh-hoáº¡t-Ä‘á»™ng)
7. [Káº¿t Quáº£ Thá»±c Nghiá»‡m](#7-káº¿t-quáº£-thá»±c-nghiá»‡m)
8. [CÃ¢u Há»i Váº¥n ÄÃ¡p](#8-cÃ¢u-há»i-váº¥n-Ä‘Ã¡p)
9. [HÆ°á»›ng Dáº«n Demo](#9-hÆ°á»›ng-dáº«n-demo)
10. [Káº¿t Luáº­n](#10-káº¿t-luáº­n)

---

## 6. QUY TRÃŒNH HOáº T Äá»˜NG

### 6.1. Tá»•ng Quan Quy TrÃ¬nh FL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEDERATED LEARNING WORKFLOW                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   ROUND 1, 2, 3, 4, 5...                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                              â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚   â”‚
â”‚   â”‚  â”‚ STEP 1   â”‚  Server khá»Ÿi táº¡o Global Model                 â”‚   â”‚
â”‚   â”‚  â”‚ INIT     â”‚  (random weights hoáº·c pre-trained)            â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                               â”‚   â”‚
â”‚   â”‚       â”‚                                                      â”‚   â”‚
â”‚   â”‚       â–¼                                                      â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚   â”‚
â”‚   â”‚  â”‚ STEP 2   â”‚  Server BROADCAST global model Ä‘áº¿n clients    â”‚   â”‚
â”‚   â”‚  â”‚ BROADCASTâ”‚  (gá»­i weights qua gRPC)                       â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                               â”‚   â”‚
â”‚   â”‚       â”‚                                                      â”‚   â”‚
â”‚   â”‚       â–¼                                                      â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚   â”‚
â”‚   â”‚  â”‚ STEP 3   â”‚  Má»—i Client TRAIN trÃªn local data             â”‚   â”‚
â”‚   â”‚  â”‚ TRAINING â”‚  (khÃ´ng chia sáº» data vá»›i ai)                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                               â”‚   â”‚
â”‚   â”‚       â”‚                                                      â”‚   â”‚
â”‚   â”‚       â–¼                                                      â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚   â”‚
â”‚   â”‚  â”‚ STEP 4   â”‚  Clients Gá»¬I updated weights vá» server        â”‚   â”‚
â”‚   â”‚  â”‚ UPLOAD   â”‚  (chá»‰ gá»­i weights, KHÃ”NG gá»­i data)            â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                               â”‚   â”‚
â”‚   â”‚       â”‚                                                      â”‚   â”‚
â”‚   â”‚       â–¼                                                      â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚   â”‚
â”‚   â”‚  â”‚ STEP 5   â”‚  Server AGGREGATE táº¥t cáº£ weights              â”‚   â”‚
â”‚   â”‚  â”‚AGGREGATE â”‚  (FedAvg / FedProx / FedOpt)                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                               â”‚   â”‚
â”‚   â”‚       â”‚                                                      â”‚   â”‚
â”‚   â”‚       â–¼                                                      â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚   â”‚
â”‚   â”‚  â”‚ STEP 6   â”‚  Cáº­p nháº­t Global Model                        â”‚   â”‚
â”‚   â”‚  â”‚ UPDATE   â”‚  â†’ Láº·p láº¡i tá»« Step 2                          â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚   â”‚
â”‚   â”‚                                                              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2. Chi Tiáº¿t Tá»«ng BÆ°á»›c

#### ğŸ“¡ STEP 1: Khá»Ÿi Táº¡o (Initialization)
```python
# Server khá»Ÿi táº¡o model vá»›i random weights
model = create_model()  # Neural Network 15â†’64â†’32â†’1

# Flower server config
server_config = fl.server.ServerConfig(num_rounds=5)
```

#### ğŸ“¤ STEP 2: Broadcast Global Model
```python
# Server gá»­i weights Ä‘áº¿n táº¥t cáº£ clients
# ThÃ´ng qua gRPC protocol cá»§a Flower framework

# Táº¡i má»—i client - nháº­n weights:
def get_parameters(self, config):
    return self.model.get_weights()
```

#### ğŸ‹ï¸ STEP 3: Local Training
```python
# Má»—i client train trÃªn LOCAL data cá»§a riÃªng mÃ¬nh
def fit(self, parameters, config):
    self.model.set_weights(parameters)  # Set global weights
    
    # Load LOCAL data (khÃ´ng chia sáº»!)
    X_train, y_train = load_local_data()
    
    # Train 3 epochs
    self.model.fit(X_train, y_train, epochs=3, batch_size=32)
    
    # Tráº£ vá» updated weights (KHÃ”NG tráº£ data!)
    return self.model.get_weights(), len(X_train), {}
```

**Äiá»ƒm quan trá»ng vá» Privacy:**
- âœ… Data **KHÃ”NG BAO GIá»œ** rá»i khá»i client
- âœ… Chá»‰ cÃ³ weights (5,057 parameters) Ä‘Æ°á»£c gá»­i Ä‘i
- âœ… KhÃ´ng thá»ƒ reverse-engineer data tá»« weights

#### ğŸ“¥ STEP 4: Upload Weights
```python
# Client gá»­i weights vá» server
# Bandwidth cáº§n: ~20KB (thay vÃ¬ MB/GB raw data)

# Weights format:
[
    layer1_weights: (15, 64),   # 960 floats
    layer1_bias: (64,),          # 64 floats
    layer2_weights: (64, 32),   # 2048 floats
    layer2_bias: (32,),          # 32 floats
    output_weights: (32, 1),    # 32 floats
    output_bias: (1,)            # 1 float
]
# Total: 5,057 parameters Ã— 4 bytes = ~20KB
```

#### ğŸ”„ STEP 5: Aggregation

**FedAvg:**
```python
def aggregate_fit(self, results):
    # TÃ­nh weighted average
    total_samples = sum(num_samples for _, num_samples, _ in results)
    
    new_weights = []
    for layer_idx in range(len(results[0][0])):
        layer_weights = sum(
            weights[layer_idx] * (num_samples / total_samples)
            for weights, num_samples, _ in results
        )
        new_weights.append(layer_weights)
    
    return new_weights
```

**FedProx:**
```python
# ThÃªm proximal term trong loss function
def local_loss(w, w_global, mu=0.01):
    return original_loss(w) + (mu/2) * ||w - w_global||Â²
```

**FedOpt:**
```python
# Server-side Adam optimizer
m = beta1 * m + (1 - beta1) * delta_w
v = beta2 * v + (1 - beta2) * delta_w**2
w_new = w - lr * m / (sqrt(v) + eps)
```

#### âœ… STEP 6: Update & Repeat
```python
# Cáº­p nháº­t global model
global_model.set_weights(aggregated_weights)

# Gá»­i metrics vá» Backend API
requests.post('http://backend:3000/api/log', json={
    'strategy': 'FedAvg',
    'round': current_round,
    'accuracy': accuracy,
    'loss': loss
})

# Láº·p láº¡i tá»« Step 2 cho round tiáº¿p theo
```

### 6.3. Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA FLOW                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   CLIENT 1 (ISP-A)          SERVER              CLIENT 2 (ISP-B)       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚ Local Data  â”‚       â”‚Global Model â”‚       â”‚ Local Data  â”‚          â”‚
â”‚   â”‚ 2000 samplesâ”‚       â”‚   w(t)      â”‚       â”‚ 2000 samplesâ”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚          â”‚                     â”‚                     â”‚                  â”‚
â”‚          â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                  â”‚
â”‚          â”‚    â”‚                â”‚                â”‚   â”‚                  â”‚
â”‚          â”‚â—„â”€â”€â”€â”¤  BROADCAST     â”‚    BROADCAST  â”œâ”€â”€â”€â–ºâ”‚                  â”‚
â”‚          â”‚    â”‚  weights       â”‚    weights    â”‚   â”‚                  â”‚
â”‚          â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                  â”‚
â”‚          â”‚                                          â”‚                  â”‚
â”‚          â–¼                                          â–¼                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚ Local Train â”‚                           â”‚ Local Train â”‚            â”‚
â”‚   â”‚ 3 epochs    â”‚                           â”‚ 3 epochs    â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚          â”‚                                          â”‚                  â”‚
â”‚          â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                  â”‚
â”‚          â”‚    â”‚                                â”‚   â”‚                  â”‚
â”‚          â”œâ”€â”€â”€â”€â–º      UPLOAD weights only      â—„â”€â”€â”€â”€â”¤                  â”‚
â”‚          â”‚    â”‚      (~20KB per client)       â”‚   â”‚                  â”‚
â”‚          â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                  â”‚
â”‚          â”‚                     â”‚                   â”‚                  â”‚
â”‚          â”‚                     â–¼                   â”‚                  â”‚
â”‚          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚                  â”‚
â”‚          â”‚              â”‚ AGGREGATION â”‚            â”‚                  â”‚
â”‚          â”‚              â”‚ FedAvg/Prox â”‚            â”‚                  â”‚
â”‚          â”‚              â”‚    /Opt     â”‚            â”‚                  â”‚
â”‚          â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â”‚                  â”‚
â”‚          â”‚                     â”‚                   â”‚                  â”‚
â”‚          â”‚                     â–¼                   â”‚                  â”‚
â”‚          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚                  â”‚
â”‚          â”‚              â”‚   w(t+1)    â”‚            â”‚                  â”‚
â”‚          â”‚              â”‚ New Global  â”‚            â”‚                  â”‚
â”‚          â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                  â”‚
â”‚                                                                         â”‚
â”‚   ğŸ”’ DATA NEVER LEAVES THE CLIENT!                                     â”‚
â”‚   ğŸ“¤ ONLY WEIGHTS ARE SHARED                                           â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Káº¾T QUáº¢ THá»°C NGHIá»†M

### 7.1. Cáº¥u HÃ¬nh Thá»±c Nghiá»‡m

| Tham Sá»‘ | GiÃ¡ Trá»‹ |
|---------|---------|
| Sá»‘ rounds | 5 |
| Sá»‘ clients má»—i strategy | 3 |
| Samples/client | 2000 |
| Local epochs | 3 |
| Batch size | 32 |
| Learning rate | 0.001 |
| FedProx Î¼ | 0.01 |

### 7.2. Káº¿t Quáº£ Accuracy

| Round | FedAvg | FedProx | FedOpt |
|-------|--------|---------|--------|
| 1 | 63.08% | 64.40% | 64.37% |
| 2 | 64.62% | 66.31% | 64.69% |
| 3 | 65.44% | 66.60% | 65.40% |
| 4 | 64.90% | 67.02% | 66.56% |
| 5 | **65.94%** | **66.77%** | **66.81%** |

### 7.3. Nháº­n XÃ©t

1. **FedProx** cÃ³ performance á»•n Ä‘á»‹nh nháº¥t vá»›i Non-IID data
2. **FedOpt** há»™i tá»¥ nhanh á»Ÿ rounds cuá»‘i
3. **FedAvg** Ä‘Æ¡n giáº£n nhÆ°ng váº«n Ä‘áº¡t káº¿t quáº£ tá»‘t
4. Táº¥t cáº£ Ä‘á»u Ä‘áº¡t accuracy > 65% sau 5 rounds

### 7.4. So SÃ¡nh vá»›i Centralized ML

| PhÆ°Æ¡ng PhÃ¡p | Accuracy | Privacy | Bandwidth |
|-------------|----------|---------|-----------|
| Centralized ML | ~70-75% | âŒ KÃ©m | âŒ Cao |
| Federated Learning | ~65-67% | âœ… Tá»‘t | âœ… Tháº¥p |

**Trade-off:** Giáº£m ~5-8% accuracy Ä‘á»ƒ Ä‘á»•i láº¥y privacy vÃ  giáº£m bandwidth.

---

## 8. CÃ‚U Há»I Váº¤N ÄÃP

### 8.1. CÃ¢u Há»i Vá» Federated Learning

#### â“ Q1: Federated Learning khÃ¡c gÃ¬ vá»›i Machine Learning truyá»n thá»‘ng?

**Tráº£ lá»i:**
| TiÃªu ChÃ­ | ML Truyá»n Thá»‘ng | Federated Learning |
|----------|-----------------|-------------------|
| **Data** | Táº­p trung 1 nÆ¡i | PhÃ¢n tÃ¡n táº¡i clients |
| **Training** | Server train | Clients train |
| **Privacy** | Dá»¯ liá»‡u bá»‹ thu tháº­p | Dá»¯ liá»‡u giá»¯ táº¡i nguá»“n |
| **Bandwidth** | Gá»­i raw data (MB-GB) | Gá»­i weights (~KB) |
| **Use case** | Single organization | Cross-organization |

---

#### â“ Q2: Táº¡i sao cáº§n Federated Learning cho bÃ i toÃ¡n DDoS?

**Tráº£ lá»i:**
1. **Privacy cá»§a ISP**: CÃ¡c ISP khÃ´ng muá»‘n chia sáº» traffic data
2. **Compliance**: TuÃ¢n thá»§ GDPR, data protection laws
3. **Bandwidth**: KhÃ´ng cáº§n truyá»n TB dá»¯ liá»‡u traffic
4. **Real-time**: Má»—i ISP cÃ³ thá»ƒ train ngay trÃªn local data
5. **Collaboration**: Nhiá»u ISP há»£p tÃ¡c mÃ  khÃ´ng lá»™ data

---

#### â“ Q3: Data khÃ´ng Ä‘Æ°á»£c gá»­i Ä‘i, váº­y lÃ m sao model há»c Ä‘Æ°á»£c?

**Tráº£ lá»i:**
```
Model há»c thÃ´ng qua WEIGHTS, khÃ´ng pháº£i DATA:

1. Client nháº­n global weights: w_global
2. Client train trÃªn local data: w_local = train(w_global, local_data)
3. Client gá»­i w_local vá» server
4. Server aggregate: w_new = average(w_local_1, w_local_2, w_local_3)

â†’ Weights chá»©a "knowledge" há»c Ä‘Æ°á»£c tá»« data
â†’ KhÃ´ng thá»ƒ reverse-engineer data tá»« weights
â†’ Data KHÃ”NG BAO GIá»œ rá»i khá»i client
```

---

#### â“ Q4: FedAvg, FedProx, FedOpt khÃ¡c nhau nhÆ° tháº¿ nÃ o?

**Tráº£ lá»i:**

| Thuáº­t ToÃ¡n | CÃ¡ch Aggregate | Khi NÃ o DÃ¹ng |
|------------|----------------|--------------|
| **FedAvg** | Simple weighted average | Dá»¯ liá»‡u IID, Ä‘Æ¡n giáº£n |
| **FedProx** | + Proximal term (Î¼) | Dá»¯ liá»‡u Non-IID |
| **FedOpt** | + Server-side Adam | Cáº§n converge nhanh |

**VÃ­ dá»¥:**
- FedAvg: `w_new = 0.33*w1 + 0.33*w2 + 0.34*w3`
- FedProx: ThÃªm penalty `Î¼/2 * ||w - w_global||Â²` vÃ o loss
- FedOpt: Server dÃ¹ng Adam optimizer thay vÃ¬ simple average

---

### 8.2. CÃ¢u Há»i Vá» Há»‡ Thá»‘ng

#### â“ Q5: Há»‡ thá»‘ng cÃ³ bao nhiÃªu container?

**Tráº£ lá»i:**
```
14 containers total:
â”œâ”€â”€ 1 Frontend Dashboard (React)
â”œâ”€â”€ 1 Backend API (Node.js)
â”œâ”€â”€ 3 FL Servers (FedAvg, FedProx, FedOpt)
â””â”€â”€ 9 Clients (3 per strategy)
```

---

#### â“ Q6: Táº¡i sao dÃ¹ng Docker?

**Tráº£ lá»i:**
1. **Isolation**: Má»—i service cháº¡y Ä‘á»™c láº­p
2. **Reproducibility**: MÃ´i trÆ°á»ng giá»‘ng nhau má»i nÆ¡i
3. **Scalability**: Dá»… scale up/down
4. **Deployment**: Deploy Ä‘Æ¡n giáº£n vá»›i `docker compose up`
5. **Development**: Team dá»… collaborate

---

#### â“ Q7: Flower framework lÃ  gÃ¬?

**Tráº£ lá»i:**
**Flower (flwr)** lÃ  framework FL phá»• biáº¿n nháº¥t:
- Developed by Adap GmbH
- Open-source, production-ready
- Support TensorFlow, PyTorch, scikit-learn
- Built-in strategies: FedAvg, FedProx, FedOpt
- gRPC communication

```python
# Server
fl.server.start_server(strategy=fl.server.strategy.FedAvg())

# Client
fl.client.start_numpy_client(server_address="...", client=MyClient())
```

---

### 8.3. CÃ¢u Há»i Vá» Security & Privacy

#### â“ Q8: Federated Learning cÃ³ thá»±c sá»± báº£o máº­t khÃ´ng?

**Tráº£ lá»i:**
**Æ¯u Ä‘iá»ƒm:**
- âœ… Data khÃ´ng rá»i khá»i client
- âœ… Chá»‰ share model weights
- âœ… Giáº£m attack surface

**Háº¡n cháº¿ cáº§n biáº¿t:**
- âš ï¸ **Gradient leakage attack**: CÃ³ thá»ƒ infer data tá»« gradients
- âš ï¸ **Model inversion attack**: Reconstruct training data
- âš ï¸ **Membership inference**: Kiá»ƒm tra data cÃ³ trong training set

**Giáº£i phÃ¡p tÄƒng cÆ°á»ng:**
- Differential Privacy
- Secure Aggregation
- Homomorphic Encryption

---

#### â“ Q9: LÃ m sao Ä‘áº£m báº£o client khÃ´ng gá»­i weights giáº£?

**Tráº£ lá»i:**
ÄÃ¢y lÃ  váº¥n Ä‘á» **Byzantine fault tolerance**:

1. **Trusted environment**: Giáº£ Ä‘á»‹nh clients Ä‘Ã¡ng tin cáº­y
2. **Byzantine-resilient aggregation**: Loáº¡i bá» outliers
3. **Secure aggregation**: MÃ£ hÃ³a weights
4. **Model validation**: Server validate weights trÆ°á»›c khi aggregate

---

### 8.4. CÃ¢u Há»i Vá» Performance

#### â“ Q10: Accuracy 65-67% cÃ³ Ä‘á»§ tá»‘t khÃ´ng?

**Tráº£ lá»i:**
**So sÃ¡nh:**
| Method | Accuracy | Trade-off |
|--------|----------|-----------|
| Centralized ML | 70-75% | Privacy âŒ |
| Federated Learning | 65-67% | Privacy âœ… |

**Giáº£i thÃ­ch:**
- FL sacrifice ~5-8% accuracy for privacy
- Trong thá»±c táº¿, cÃ³ thá»ƒ tÄƒng accuracy báº±ng:
  - TÄƒng sá»‘ rounds
  - TÄƒng local epochs
  - ThÃªm data augmentation
  - Fine-tune hyperparameters

---

#### â“ Q11: Táº¡i sao chá»n 5 rounds?

**Tráº£ lá»i:**
- Demo purpose: 5 rounds Ä‘á»§ Ä‘á»ƒ tháº¥y convergence
- Production: ThÆ°á»ng 10-100 rounds
- Trade-off: Communication cost vs accuracy
- Empirically: Accuracy plateau sau ~5-10 rounds vá»›i dataset nÃ y

---

## 9. HÆ¯á»šNG DáºªN DEMO

### 9.1. Khá»Ÿi Äá»™ng Há»‡ Thá»‘ng

```powershell
# BÆ°á»›c 1: VÃ o thÆ° má»¥c project
cd E:\DEVcodon\Projects\ddos-fl-system

# BÆ°á»›c 2: Build vÃ  start táº¥t cáº£ containers
docker compose up -d

# BÆ°á»›c 3: Xem logs real-time
docker compose logs -f
```

### 9.2. Truy Cáº­p Dashboard

| URL | Má»¥c ÄÃ­ch |
|-----|----------|
| http://localhost:3001 | Frontend Dashboard |
| http://localhost:3000 | Backend API |

### 9.3. Demo Flow

1. **Má»Ÿ Dashboard** â†’ http://localhost:3001
2. **Tab Tá»•ng Quan**: Xem overview, tech stack
3. **Tab So SÃ¡nh**: Xem biá»ƒu Ä‘á»“ accuracy 3 strategies
4. **Tab Training Logs**: Xem chi tiáº¿t tá»«ng round
5. **Tab Quy TrÃ¬nh**: Animation giáº£i thÃ­ch FL

### 9.4. CÃ¡c Äiá»ƒm Demo Quan Trá»ng

1. **Real-time updates**: Metrics cáº­p nháº­t má»—i 2s
2. **Privacy**: Highlight ráº±ng data khÃ´ng rá»i client
3. **Comparison**: So sÃ¡nh 3 strategies
4. **Architecture**: Giáº£i thÃ­ch Docker containers

### 9.5. Dá»«ng Há»‡ Thá»‘ng

```powershell
# Dá»«ng táº¥t cáº£ containers
docker compose down

# Dá»«ng vÃ  xÃ³a volumes
docker compose down -v
```

---

## 10. Káº¾T LUáº¬N

### 10.1. ÄÃ³ng GÃ³p Cá»§a Äá»“ Ãn

1. âœ… XÃ¢y dá»±ng há»‡ thá»‘ng FL hoÃ n chá»‰nh cho DDoS detection
2. âœ… So sÃ¡nh 3 thuáº­t toÃ¡n: FedAvg, FedProx, FedOpt
3. âœ… Dashboard giÃ¡m sÃ¡t real-time
4. âœ… Containerize vá»›i Docker cho dá»… deploy
5. âœ… Documentation Ä‘áº§y Ä‘á»§

### 10.2. Háº¡n Cháº¿

1. âŒ ChÆ°a implement Differential Privacy
2. âŒ ChÆ°a cÃ³ Secure Aggregation
3. âŒ Dataset cÃ²n nhá» (synthetic data)
4. âŒ ChÆ°a test vá»›i real network traffic

### 10.3. HÆ°á»›ng PhÃ¡t Triá»ƒn

1. ğŸ”œ ThÃªm Differential Privacy
2. ğŸ”œ Implement Secure Aggregation
3. ğŸ”œ Test vá»›i real DDoS datasets (CIC-DDoS2019)
4. ğŸ”œ Deploy lÃªn cloud (AWS/GCP)
5. ğŸ”œ ThÃªm more strategies (FedYogi, SCAFFOLD)

### 10.4. TÃ i Liá»‡u Tham Kháº£o

1. McMahan, H. B., et al. (2017). "Communication-Efficient Learning of Deep Networks from Decentralized Data"
2. Li, T., et al. (2020). "Federated Optimization in Heterogeneous Networks" (FedProx)
3. Reddi, S., et al. (2021). "Adaptive Federated Optimization" (FedOpt)
4. Flower Framework: https://flower.dev/
5. TensorFlow Federated: https://www.tensorflow.org/federated

---

## ğŸ“ LIÃŠN Há»†

Náº¿u cÃ³ tháº¯c máº¯c vá» Ä‘á»“ Ã¡n, vui lÃ²ng liÃªn há»‡ qua:
- Email: [your-email]
- GitHub: [your-github]

---

*Äá»“ Ã¡n tá»‘t nghiá»‡p - Há»‡ thá»‘ng phÃ¡t hiá»‡n DDoS sá»­ dá»¥ng Federated Learning*
*Â© 2025*
